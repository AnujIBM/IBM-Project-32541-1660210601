import pandas as pd

import numpy as np 

import seaborn as sns

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
#load/read dataset
data=pd.read_csv(r"C:\Users\rnsam\Downloads\abalone.csv")
data
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7
...	...	...	...	...	...	...	...	...	...
4172	F	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	M	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	M	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	F	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	M	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
4177 rows × 9 columns

#print top 5 rows
data.head()
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.150	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.070	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.210	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.155	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.055	7
#print last 5 rows
data.tail()
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
4172	F	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	M	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	M	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	F	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	M	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
data.shape
(4177, 9)
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4177 entries, 0 to 4176
Data columns (total 9 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   Sex             4177 non-null   object 
 1   Length          4177 non-null   float64
 2   Diameter        4177 non-null   float64
 3   Height          4177 non-null   float64
 4   Whole weight    4177 non-null   float64
 5   Shucked weight  4177 non-null   float64
 6   Viscera weight  4177 non-null   float64
 7   Shell weight    4177 non-null   float64
 8   Rings           4177 non-null   int64  
dtypes: float64(7), int64(1), object(1)
memory usage: 293.8+ KB
data.describe()
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
count	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000
mean	0.523992	0.407881	0.139516	0.828742	0.359367	0.180594	0.238831	9.933684
std	0.120093	0.099240	0.041827	0.490389	0.221963	0.109614	0.139203	3.224169
min	0.075000	0.055000	0.000000	0.002000	0.001000	0.000500	0.001500	1.000000
25%	0.450000	0.350000	0.115000	0.441500	0.186000	0.093500	0.130000	8.000000
50%	0.545000	0.425000	0.140000	0.799500	0.336000	0.171000	0.234000	9.000000
75%	0.615000	0.480000	0.165000	1.153000	0.502000	0.253000	0.329000	11.000000
max	0.815000	0.650000	1.130000	2.825500	1.488000	0.760000	1.005000	29.000000
data.columns
Index(['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',
       'Viscera weight', 'Shell weight', 'Rings'],
      dtype='object')
numerical_features = data.select_dtypes(include=[np.number]).columns
categorical_features = data.select_dtypes(include=[np.object]).columns
univariate analysis

#histogram
data.hist(figsize=(20,10), grid=False, layout=(2, 4), bins = 30)
array([[<AxesSubplot:title={'center':'Length'}>,
        <AxesSubplot:title={'center':'Diameter'}>,
        <AxesSubplot:title={'center':'Height'}>,
        <AxesSubplot:title={'center':'Whole weight'}>],
       [<AxesSubplot:title={'center':'Shucked weight'}>,
        <AxesSubplot:title={'center':'Viscera weight'}>,
        <AxesSubplot:title={'center':'Shell weight'}>,
        <AxesSubplot:title={'center':'Rings'}>]], dtype=object)

sns.countplot(x = 'Sex', data = data, palette="Set2")
<AxesSubplot:xlabel='Sex', ylabel='count'>

#BOX PLOTS
plt.boxplot(data['Rings'])
{'whiskers': [<matplotlib.lines.Line2D at 0x21789398b20>,
  <matplotlib.lines.Line2D at 0x21789398df0>],
 'caps': [<matplotlib.lines.Line2D at 0x217893a2100>,
  <matplotlib.lines.Line2D at 0x217893a23d0>],
 'boxes': [<matplotlib.lines.Line2D at 0x21789398850>],
 'medians': [<matplotlib.lines.Line2D at 0x217893a26a0>],
 'fliers': [<matplotlib.lines.Line2D at 0x217893a2970>],
 'means': []}

abs=data['Sex'].value_counts()
abs
M    1528
I    1342
F    1307
Name: Sex, dtype: int64
#distribution plot
sns.distplot(data.Height)
<AxesSubplot:xlabel='Height', ylabel='Density'>

Bivariate Analysis
#scatter plot
sns.scatterplot(data.Rings,data.Diameter)
<AxesSubplot:xlabel='Rings', ylabel='Diameter'>

#jointplot
sns.jointplot(data.Rings,data.Diameter)
<seaborn.axisgrid.JointGrid at 0x21791f0c9a0>

#boxplot
sns.boxplot(data['Whole weight'])
<AxesSubplot:xlabel='Whole weight'>

#point plot
sns.pointplot(x='Height',y='Whole weight',data=data,color='darkorange')
<AxesSubplot:xlabel='Height', ylabel='Whole weight'>

Multi - Variate Analysis
sns.pairplot(data[numerical_features])
<seaborn.axisgrid.PairGrid at 0x2179225ffd0>

#heat map
sns.heatmap(data[numerical_features].corr(), annot=True)
<AxesSubplot:>

descriptive statistics
import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sns
#load/read dataset
data1=pd.read_csv(r"C:\Users\rnsam\Downloads\abalone.csv")
data1
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7
...	...	...	...	...	...	...	...	...	...
4172	F	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	M	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	M	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	F	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	M	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
4177 rows × 9 columns

measure of central tendancy mean median mode
data1.mean()
Length            0.523992
Diameter          0.407881
Height            0.139516
Whole weight      0.828742
Shucked weight    0.359367
Viscera weight    0.180594
Shell weight      0.238831
Rings             9.933684
dtype: float64
data1.median()
Length            0.5450
Diameter          0.4250
Height            0.1400
Whole weight      0.7995
Shucked weight    0.3360
Viscera weight    0.1710
Shell weight      0.2340
Rings             9.0000
dtype: float64
data1.mode()
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.550	0.45	0.15	0.2225	0.175	0.1715	0.275	9.0
1	NaN	0.625	NaN	NaN	NaN	NaN	NaN	NaN	NaN
skewness
data1.skew()
Length           -0.639873
Diameter         -0.609198
Height            3.128817
Whole weight      0.530959
Shucked weight    0.719098
Viscera weight    0.591852
Shell weight      0.620927
Rings             1.114102
dtype: float64
print(sns.distplot(data1['Height'],color='red'))
AxesSubplot(0.125,0.125;0.775x0.755)

data1.kurt()
Length             0.064621
Diameter          -0.045476
Height            76.025509
Whole weight      -0.023644
Shucked weight     0.595124
Viscera weight     0.084012
Shell weight       0.531926
Rings              2.330687
dtype: float64
IQR
quantile=data1['Rings'].quantile(q=[0.75,0.25])
quantile
0.75    11.0
0.25     8.0
Name: Rings, dtype: float64
quantile=data1['Diameter'].quantile(q=[0.75,0.25])
quantile
0.75    0.48
0.25    0.35
Name: Diameter, dtype: float64
#all data sets cal
quantile=data1.quantile(q=[0.75,0.25])
quantile
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0.75	0.615	0.48	0.165	1.1530	0.502	0.2530	0.329	11.0
0.25	0.450	0.35	0.115	0.4415	0.186	0.0935	0.130	8.0
IQR=quantile.iloc[0]-quantile.iloc[1]
IQR
Length            0.1650
Diameter          0.1300
Height            0.0500
Whole weight      0.7115
Shucked weight    0.3160
Viscera weight    0.1595
Shell weight      0.1990
Rings             3.0000
dtype: float64
x=data1[['Height','Diameter']]
x
Height	Diameter
0	0.095	0.365
1	0.090	0.265
2	0.135	0.420
3	0.125	0.365
4	0.080	0.255
...	...	...
4172	0.165	0.450
4173	0.135	0.440
4174	0.205	0.475
4175	0.150	0.485
4176	0.195	0.555
4177 rows × 2 columns

#normalization
from sklearn.preprocessing import MinMaxScaler
min_max=MinMaxScaler(feature_range=(0,1))
min_max.fit_transform(x)
array([[0.0840708 , 0.5210084 ],
       [0.07964602, 0.35294118],
       [0.11946903, 0.61344538],
       ...,
       [0.18141593, 0.70588235],
       [0.13274336, 0.72268908],
       [0.17256637, 0.84033613]])
var and std deviation
#variance
data1.var()
Length             0.014422
Diameter           0.009849
Height             0.001750
Whole weight       0.240481
Shucked weight     0.049268
Viscera weight     0.012015
Shell weight       0.019377
Rings             10.395266
dtype: float64
#std deviation
data1.std()
Length            0.120093
Diameter          0.099240
Height            0.041827
Whole weight      0.490389
Shucked weight    0.221963
Viscera weight    0.109614
Shell weight      0.139203
Rings             3.224169
dtype: float64
scaling
data1.columns
Index(['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',
       'Viscera weight', 'Shell weight', 'Rings'],
      dtype='object')
r=data1[['Shucked weight','Whole weight']]
r
Shucked weight	Whole weight
0	0.2245	0.5140
1	0.0995	0.2255
2	0.2565	0.6770
3	0.2155	0.5160
4	0.0895	0.2050
...	...	...
4172	0.3700	0.8870
4173	0.4390	0.9660
4174	0.5255	1.1760
4175	0.5310	1.0945
4176	0.9455	1.9485
4177 rows × 2 columns

from sklearn.preprocessing import StandardScaler
scale=StandardScaler()
st_scale=scale.fit_transform(r)
st_scale
array([[-0.60768536, -0.64189823],
       [-1.17090984, -1.23027711],
       [-0.4634999 , -0.30946926],
       ...,
       [ 0.74855917,  0.70821206],
       [ 0.77334105,  0.54199757],
       [ 2.64099341,  2.28368063]])
#range
data1.max()
Sex                    M
Length             0.815
Diameter            0.65
Height              1.13
Whole weight      2.8255
Shucked weight     1.488
Viscera weight      0.76
Shell weight       1.005
Rings                 29
dtype: object
data1.min()
Sex                    F
Length             0.075
Diameter           0.055
Height               0.0
Whole weight       0.002
Shucked weight     0.001
Viscera weight    0.0005
Shell weight      0.0015
Rings                  1
dtype: object
5. Check for Missing values and deal with them.
#checking null values/MISSING VALUE
data1.isna()
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	False	False	False	False	False	False	False	False	False
1	False	False	False	False	False	False	False	False	False
2	False	False	False	False	False	False	False	False	False
3	False	False	False	False	False	False	False	False	False
4	False	False	False	False	False	False	False	False	False
...	...	...	...	...	...	...	...	...	...
4172	False	False	False	False	False	False	False	False	False
4173	False	False	False	False	False	False	False	False	False
4174	False	False	False	False	False	False	False	False	False
4175	False	False	False	False	False	False	False	False	False
4176	False	False	False	False	False	False	False	False	False
4177 rows × 9 columns

#CATEGORICAL FORM
data.isna().any()
Sex               False
Length            False
Diameter          False
Height            False
Whole weight      False
Shucked weight    False
Viscera weight    False
Shell weight      False
Rings             False
dtype: bool
#NUMBERICAL FORM
data.isna().sum()
Sex               0
Length            0
Diameter          0
Height            0
Whole weight      0
Shucked weight    0
Viscera weight    0
Shell weight      0
Rings             0
dtype: int64
6. Find the outliers and replace the outliers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#load/read dataset
data=pd.read_csv(r"C:\Users\rnsam\Downloads\abalone.csv")
data
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7
...	...	...	...	...	...	...	...	...	...
4172	F	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	M	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	M	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	F	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	M	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
4177 rows × 9 columns

data = pd.get_dummies(data)
dummy_data = data.copy()
data.boxplot( rot = 90, figsize=(20,5))
<AxesSubplot:>

var = 'Viscera weight'
plt.scatter(x = data[var], y = data['Height'],)
plt.grid(True)

# outliers removal
data.drop(data[(data['Viscera weight']> 0.5) & (data['Height'] < 20)].index, inplace=True)
data.drop(data[(data['Viscera weight']<0.5) & (data['Height'] > 25)].index, inplace=True)
var = 'Shell weight'
plt.scatter(x = data[var], y = data['Height'],)
plt.grid(True)

data.drop(data[(data['Shell weight']> 0.6) & (data['Height'] < 25)].index, inplace=True)
data.drop(data[(data['Shell weight']<0.8) & (data['Height'] > 25)].index, inplace=True)
var = 'Shucked weight'
plt.scatter(x = data[var], y = data['Height'],)
plt.grid(True)

data.drop(data[(data['Shucked weight']>= 1) & (data['Height'] < 20)].index, inplace=True)
data.drop(data[(data['Shucked weight']<1) & (data['Height'] > 20)].index, inplace=True)
var = 'Whole weight'
plt.scatter(x = data[var], y = data['Height'],)
plt.grid(True)

data.drop(data[(data['Whole weight']>= 2.5) & (data['Height'] < 25)].index, inplace=True)
data.drop(data[(data['Whole weight']<2.5) & (data['Height'] > 25)].index, inplace=True)
var = 'Diameter'
plt.scatter(x = data[var], y = data['Height'],)
plt.grid(True)

data.drop(data[(data['Length']<0.1) & (data['Height'] < 5)].index, inplace=True)
data.drop(data[(data['Length']<0.8) & (data['Height'] > 25)].index, inplace=True)
data.drop(data[(data['Length']>=0.8) & (data['Height']< 25)].index, inplace=True)
7. Check for Categorical columns and perform encoding
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#load/read dataset
data=pd.read_csv(r"C:\Users\rnsam\Downloads\abalone.csv")
data
Sex	Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7
...	...	...	...	...	...	...	...	...	...
4172	F	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	M	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	M	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	F	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	M	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
4177 rows × 9 columns

#dummy variable // convert this column into categ
data_main = pd.get_dummies(data, columns =['Sex'])
data_main
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings	Sex_F	Sex_I	Sex_M
0	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15	0	0	1
1	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7	0	0	1
2	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9	1	0	0
3	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10	0	0	1
4	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7	0	1	0
...	...	...	...	...	...	...	...	...	...	...	...
4172	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11	1	0	0
4173	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10	0	0	1
4174	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9	0	0	1
4175	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10	1	0	0
4176	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12	0	0	1
4177 rows × 11 columns

#encoding
data_main ['Sex_F'].replace ( { 0: 'no', 1 : 'yes' },inplace = True )
data_main
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings	Sex_F	Sex_I	Sex_M
0	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15	no	0	1
1	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7	no	0	1
2	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9	yes	0	0
3	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10	no	0	1
4	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7	no	1	0
...	...	...	...	...	...	...	...	...	...	...	...
4172	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11	yes	0	0
4173	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10	no	0	1
4174	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9	no	0	1
4175	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10	yes	0	0
4176	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12	no	0	1
4177 rows × 11 columns

#encoding
data_main ['Sex_M'].replace ( { 0: 'no', 1 : 'yes' },inplace = True )
data_main
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings	Sex_F	Sex_I	Sex_M
0	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15	no	0	yes
1	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7	no	0	yes
2	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9	yes	0	no
3	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10	no	0	yes
4	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7	no	1	no
...	...	...	...	...	...	...	...	...	...	...	...
4172	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11	yes	0	no
4173	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10	no	0	yes
4174	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9	no	0	yes
4175	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10	yes	0	no
4176	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12	no	0	yes
4177 rows × 11 columns

# splitting the dependent and independent variables

y = data_main['Rings']
y
0       15
1        7
2        9
3       10
4        7
        ..
4172    11
4173    10
4174     9
4175    10
4176    12
Name: Rings, Length: 4177, dtype: int64
#independent var
x=data_main.drop(columns=['Sex_I','Sex_M','Sex_F'],axis=1)
x
Length	Diameter	Height	Whole weight	Shucked weight	Viscera weight	Shell weight	Rings
0	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.1500	15
1	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.0700	7
2	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.2100	9
3	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.1550	10
4	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.0550	7
...	...	...	...	...	...	...	...	...
4172	0.565	0.450	0.165	0.8870	0.3700	0.2390	0.2490	11
4173	0.590	0.440	0.135	0.9660	0.4390	0.2145	0.2605	10
4174	0.600	0.475	0.205	1.1760	0.5255	0.2875	0.3080	9
4175	0.625	0.485	0.150	1.0945	0.5310	0.2610	0.2960	10
4176	0.710	0.555	0.195	1.9485	0.9455	0.3765	0.4950	12
4177 rows × 8 columns

names=x.columns
names
Index(['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',
       'Viscera weight', 'Shell weight', 'Rings'],
      dtype='object')
scale the independent variables
from sklearn.preprocessing import scale
x=scale(x)
x
array([[-0.57455813, -0.43214879, -1.06442415, ..., -0.72621157,
        -0.63821689,  1.57154357],
       [-1.44898585, -1.439929  , -1.18397831, ..., -1.20522124,
        -1.21298732, -0.91001299],
       [ 0.05003309,  0.12213032, -0.10799087, ..., -0.35668983,
        -0.20713907, -0.28962385],
       ...,
       [ 0.6329849 ,  0.67640943,  1.56576738, ...,  0.97541324,
         0.49695471, -0.28962385],
       [ 0.84118198,  0.77718745,  0.25067161, ...,  0.73362741,
         0.41073914,  0.02057072],
       [ 1.54905203,  1.48263359,  1.32665906, ...,  1.78744868,
         1.84048058,  0.64095986]])
x.std()
1.0
x.mean()
2.5941529136560996e-17
Split the data into training and testing¶
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
#before 
data_main.shape
(4177, 11)
#after splitting data into train and test
x_train.shape
(3341, 8)
x_test.shape
(836, 8)
y_train.shape
(3341,)
y_test.shape
(836,)
build,train,test model and using metrics
#Linear regg model
from sklearn.linear_model import LinearRegression
lin_reg=LinearRegression()
lin_reg.fit(x_train,y_train)
LinearRegression()
#linear prediction
test_pred=lin_reg.predict(x_test)
#acc
from sklearn import metrics
from sklearn.metrics import mean_squared_error
#
metrics.r2_score(y_test,test_pred)
1.0
#error

np.sqrt(mean_squared_error(y_test,test_pred))
1.7651662394568954e-15
from sklearn.linear_model import Lasso,Ridge
#intialising model
lso =Lasso(alpha=0.01,normalize=True)
#fit the model
lso.fit(x_train,y_train)
Lasso(alpha=0.01, normalize=True)
lso_pred=lso.predict(x_test)
coef=lso.coef_
coef
array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 2.64240192])
lso.alpha
0.01
#accuracy
metrics.r2_score(y_test,lso_pred)
0.9674313732018623
#error
np.sqrt(mean_squared_error(y_test,lso_pred))
0.5947313845380521
ridge
rg=Ridge(alpha=0.01,normalize=True)
#fit
rg.fit(x_train,y_train)
Ridge(alpha=0.01, normalize=True)
#prediction
rg_pred=rg.predict(x_test)
rg_pred
array([12.99743362,  8.03332441, 10.99061513,  5.01279383, 11.97518861,
       11.02113288,  7.01625123,  8.02938398,  7.02837941,  9.06086891,
        8.01346211,  7.96999329, 10.94188672,  9.00107231,  4.03813283,
        7.04892586,  7.01328708, 16.92631871,  7.08138984,  7.02434149,
        7.0162016 ,  5.04079917,  8.02569428,  8.96853566, 10.00271299,
       10.02390126,  4.99409434, 14.9544995 , 10.01605049, 13.94677466,
        7.99702466,  4.01311886,  9.04861492, 13.00731119,  7.01182579,
        7.02514595,  8.01922711,  9.02359322,  8.01225399, 11.01409051,
       14.9270949 , 11.94826793, 16.89017501, 15.90654588, 11.02518067,
       10.9678038 ,  8.02389574, 10.05028088, 11.02243201,  6.04411528,
       12.97546711,  7.0093061 , 12.92148398, 19.870397  , 11.94624005,
        7.01580552,  7.96330651,  9.9472374 ,  7.01265792,  7.00728304,
        9.01588986,  9.00262552, 10.98564692,  8.00328888,  7.02526833,
       12.00807988, 12.99030444, 16.91257234,  8.02434349, 12.04273725,
        9.01476719, 17.0200141 , 10.02119612,  6.08240668, 10.98045434,
        8.02792458,  8.03925039,  9.01772495,  8.06578323,  8.0049808 ,
        8.03480679,  6.00520744,  7.01115359, 12.98752838, 10.98438775,
        6.04882686,  9.02092002, 12.01218883,  5.00167438,  6.02512875,
       10.9895208 , 10.992628  ,  8.00190459,  6.942944  , 15.91614162,
        7.96904537, 10.9893052 ,  7.99126573, 17.91530848, 10.97884844,
       11.96596177, 13.94931011, 11.96778536, 11.00349175,  5.99475319,
        9.0307317 ,  7.0182649 ,  6.02449235, 10.93824738, 11.02994639,
       10.97410239, 11.98270197, 19.8203041 ,  9.9707535 , 13.9956871 ,
       10.01119836, 10.03601304, 12.0007564 ,  4.02851239, 11.95851004,
        7.0091106 , 12.97478964,  6.02292511,  8.03068331, 16.88289675,
       14.96027414,  9.03267158, 11.00369758,  7.02139592, 10.99653847,
        8.04707917,  7.0205958 ,  7.0444205 , 10.0252785 , 11.01406201,
        9.04016266, 10.00077134,  9.00312817,  7.99100637, 15.95850843,
       15.89457081, 12.95921183,  6.02808522,  8.99127463, 10.03213617,
        9.02193019,  8.03019562,  7.99801198,  8.01000069,  5.03591874,
        8.0279954 , 11.07871323,  5.0466508 ,  9.04245504,  8.97019165,
        6.02059601,  8.07130098,  7.00981975, 10.05096434, 10.93568749,
       11.96475532,  7.98380949,  9.02857515, 10.01735391,  5.01072662,
       15.93688973,  7.00967924,  6.01445182, 14.92685916,  9.01973573,
        9.98226553,  3.04569225,  6.04601135, 10.01112612, 11.03401499,
       11.97050969,  5.01885338,  8.01409973,  5.05589133, 12.97451044,
        9.01544522, 10.99390779,  7.02888107,  9.0063943 , 14.94775844,
        8.04685387,  9.98093074,  8.01847963,  5.03918391,  8.99567802,
       18.83292738,  9.01661037,  8.03325662,  9.02721334, 10.99201066,
        9.00384159,  8.98431288,  8.99546224,  5.00457905, 14.98674055,
        9.98839267, 13.93551281, 12.04343615,  8.03254534, 11.95351542,
       11.03076356,  5.02597199, 10.04967796, 10.98196145, 22.77763546,
        4.03752554, 10.99072189, 10.99735655, 10.0003135 ,  9.05087668,
        9.03369022,  9.03072046,  8.05674545,  5.01757237, 12.99420596,
       11.97832737, 13.95893252,  8.01161949, 10.97954087, 10.03368895,
       13.99389067, 12.96907651, 11.95431009, 12.01245281, 11.94584919,
        7.0459084 ,  7.03058781,  6.98714816, 18.85229722,  8.97046237,
       14.94289468,  8.01141285,  4.03462854,  8.01772352,  9.02163978,
        9.97999942, 11.96380963,  9.04758561,  9.02150434,  9.03269082,
        8.04495133, 11.97617374,  7.02751816, 10.98686199, 19.88922743,
        8.08193775,  7.01903091, 12.01610461, 12.92174534, 11.00219528,
       13.94207081,  6.03136957,  8.03306191,  9.03227716, 11.07376157,
        9.03231715,  8.02099156,  9.03054412, 10.92361605, 19.81744963,
        8.99605152, 11.94272692,  7.00882897,  6.04286386,  7.00526388,
       10.99495554,  7.03323371,  8.98343832,  8.98619028,  5.03179432,
        9.03993711,  9.08205349,  8.99580268, 10.00592884, 10.9729742 ,
        7.00731118, 11.99250809,  7.99557631,  9.99197192, 11.96370222,
       12.97149533,  8.9745594 , 10.01328194, 10.98756622, 10.05311421,
        8.00484305,  9.00285905,  4.0457079 ,  9.02307439,  7.00949367,
        7.9927648 ,  6.02042045,  6.00662244,  9.02817142, 12.06549001,
        9.97666913,  8.00738697, 14.95687419, 10.00163938, 12.91985844,
        9.97977494,  8.03211969, 10.00380023, 13.94617283,  8.02766628,
        9.00907956, 11.97898722,  8.0094445 ,  8.05632397, 12.07316984,
        9.05379288,  9.98647222,  7.00082545,  8.01550346, 10.00985439,
       11.00705677, 13.92077339,  9.99356613, 10.00585652, 14.94312301,
        8.03371937,  9.01072014,  9.02594593, 22.84278481,  9.9971433 ,
        8.01962283,  8.06680382,  5.00421732,  9.05633924, 10.01119412,
       12.96193577,  7.00186994,  8.02192057, 11.0074515 , 12.95301986,
        9.01799877, 11.95753774, 10.9944035 , 10.03025309, 10.04322092,
        6.02827923,  5.02567272, 11.99864147, 14.96047398,  7.01487652,
        5.03005934,  9.9832881 , 10.04822667, 12.94212746, 10.01698195,
       10.97216427, 10.01803923,  8.99970221,  7.03018727, 12.94672196,
        9.97564198, 10.00350873,  7.02656018,  9.00005709, 12.93870217,
        5.02253561,  6.01148703,  9.00016676,  7.02176107, 10.98424325,
       10.99574483,  7.01291754, 13.9512958 ,  7.01986634, 16.90954567,
       12.01703784, 12.02335782,  8.0065326 , 11.97808302,  5.98254087,
        8.01750545, 13.92577214, 10.9762252 , 14.95477756,  5.02719121,
       11.00435908, 11.09386977,  8.02177694,  7.0199003 ,  6.99506197,
       14.02324541,  8.04397364, 13.93227249, 13.9153264 , 20.94100773,
        7.99579985, 11.00443251,  8.03230772, 11.00394936, 18.82919172,
        8.0168003 ,  8.99596449,  7.0011987 , 10.06432605,  8.99250018,
       10.01094623,  5.02725172, 18.86448606, 15.99190194,  9.01380951,
        9.99716382,  7.01765498, 10.9913831 , 11.98532371, 14.96504317,
       11.00829695, 19.85265306, 11.99027479, 13.02605625, 10.00719234,
        8.02391723,  9.99171238, 10.99555301,  9.05106268,  5.02506174,
        9.9926549 , 10.99286467, 11.00739712, 11.99227465, 15.94943974,
        9.01844445, 10.99516072,  8.99888977, 13.97942575, 13.90514631,
       10.96854975, 11.02244631,  9.02463084, 11.95617007, 10.98918666,
        7.02860217,  6.02179533, 12.02620432,  8.99301761,  7.0009413 ,
        6.03064444, 10.05623557,  4.03607458, 14.90841382, 10.02037372,
        3.03455546,  5.04372904,  9.01554423,  6.02326203,  4.90129959,
       10.94951931, 11.00765512,  9.01560447,  8.01969358, 12.9828454 ,
        7.01090422, 14.0218668 ,  7.02733344,  8.0517933 , 11.9510438 ,
       10.05167845, 10.92052497, 14.08220178, 15.88483901,  6.05093187,
        9.94888186, 10.00922859,  8.00237158, 10.09150754,  8.02220336,
       10.96594336,  7.02645521,  5.06339519,  8.97212751, 10.00647237,
       11.96669439,  9.06456592,  8.01536441, 11.99424381, 15.95431474,
       11.02464399,  9.02488881, 12.93933988,  8.00775463, 10.02318909,
        3.04117053,  7.04625958, 10.04674707, 10.01609617,  6.02488245,
        6.03456333,  8.97996849,  6.01092189, 12.96726464,  9.01397207,
        7.01188187, 17.88593518,  5.04549659, 10.0407626 ,  9.03172665,
        9.03378181,  9.043895  , 17.92088415,  8.98145936,  6.9832683 ,
        9.01682198,  8.02233714, 17.92127688, 14.97870656,  9.98846813,
        8.01229444, 11.9731053 , 11.96664928, 14.98862469,  8.04636739,
        8.05226599,  9.0195148 ,  9.03720764,  9.99917361,  6.02797845,
       15.95545312,  9.98236405,  9.99695314,  7.03864669,  7.037134  ,
        8.00535927,  7.99868984,  8.99072134,  9.02642662,  5.00990793,
        9.01516684, 10.04292302, 12.00693382,  9.96411255, 13.00306702,
        9.96691662, 11.96525728,  7.9930812 , 12.00715251,  5.04352956,
        9.0313521 ,  8.00521392,  8.01325596, 10.01102769, 10.98106563,
       10.0211135 , 16.92985631, 11.92600954, 11.03831775,  8.05711323,
        8.00316983,  8.01131533,  9.01227981,  9.02811074, 11.96547588,
       12.97809149, 11.95376232, 10.03706493,  9.0050426 , 12.96588073,
        6.00159255, 10.95064454,  8.98716631, 14.90739785,  8.00514557,
       12.96036761,  8.01483993,  8.01242851,  8.03257501,  9.00494007,
        8.0030581 , 10.01928694,  9.02060444, 11.01161099,  3.03977748,
        8.10179301,  6.0084158 ,  5.99905978,  6.01207262,  8.99503645,
        9.00441128, 17.07608489, 10.01279371,  8.02392113, 12.99021615,
        6.98312758,  9.01412924, 13.93084861,  7.01944204, 12.98403234,
       10.01299942,  4.01308971, 10.95165838,  6.99527058, 10.00631042,
        8.01940475, 11.03885512, 10.0074785 , 11.94872492, 10.03773995,
        9.01784725,  7.05639769,  9.94276291,  9.99483778, 11.01826334,
        6.03271269, 11.04031152, 11.96207152,  9.00570885, 12.95765928,
       12.02683565, 11.94299464,  7.01910536, 13.02484987, 15.89037299,
       10.98240267, 11.02523724,  6.02982425,  8.03854048, 11.98389974,
        9.05332183,  7.03716283, 12.02407907,  9.97967654, 11.96801789,
       12.93813705,  7.00988086, 11.92626853,  7.03647225, 10.94735871,
       10.98594571,  5.0112408 , 10.08284089,  8.02570397,  9.01918749,
       13.92047653, 10.00446836, 11.98621607,  9.01753927,  9.01548101,
       10.97587404,  6.03553517, 13.92240703, 12.99679811, 17.85487256,
        7.02060153,  7.03607488,  9.97161377,  8.03118368,  7.00589871,
        9.05956634,  8.97844821,  8.99090332, 10.01718443, 15.94806972,
       13.9419783 ,  8.00313286, 19.87337434, 14.9243384 , 12.97490134,
        8.01731995,  9.98474751,  6.98543485,  6.01787161,  9.04268736,
        9.99049391, 10.04141771, 12.06262692, 11.02429219,  5.01654986,
        7.99877447,  8.01585524,  6.98929851,  7.009246  , 10.09881087,
       14.96580742,  8.03036757, 10.01150335,  9.99713313,  9.00925274,
        7.01675044,  9.04644648, 11.00262844,  9.03672195, 18.95112718,
        9.98937725, 10.02723937,  9.05293106, 13.01603922,  7.02295791,
        9.02962379, 28.70820937, 10.0545313 ,  8.00216989,  8.98614848,
        6.02724654, 10.01808796,  9.98929404, 19.86130967,  9.00448476,
        8.06792906, 10.97190991,  6.04993926, 11.03655512, 12.98082671,
        6.00847287,  5.00991432,  6.98454025,  9.04055694,  7.99250394,
        7.04443724, 13.94064956,  8.01922508,  7.01426327,  7.03196407,
       19.85247623,  4.02839922,  9.98753291,  6.02361135, 13.00556694,
        6.98276319, 10.95098145,  8.98625838, 18.85760485,  9.04212048,
        7.02827075, 11.00812045, 10.03620528, 18.88329494,  8.02222973,
       15.89112247,  7.01859174, 10.94676379,  7.02611774,  6.00763159,
       11.99485983,  8.99676203,  9.94697146,  6.02336456, 11.9693553 ,
        7.01206573,  9.00786217,  8.0131315 ,  7.04377785, 10.96313107,
        8.04245723,  8.03700967,  7.99361456, 10.99532322,  9.96835539,
       11.99993773,  7.05361424,  7.0104977 ,  8.02121188, 20.86857036,
        7.00260034,  4.05587647,  7.01797359,  6.00666347, 11.97766011,
        9.99700955,  9.03818012,  7.00305653,  9.0550333 ,  6.04063947,
        7.00610205, 10.9624588 , 11.04430291, 10.98710406,  8.02067655,
       10.99045445,  6.02994391, 21.81706633,  9.95481909,  9.02646161,
       16.94775647, 13.96214115,  7.01737399,  7.00560767, 15.85717596,
        8.99702782,  8.03365857, 12.91624437,  9.99956532, 11.97782862,
        8.03379068, 10.99703979, 12.03510889,  9.04648732, 13.97952365,
        9.00075473, 11.00885715,  9.02196016,  3.03501869, 13.93775473,
        9.99841247,  7.02660014,  7.03642753,  6.00772001,  6.99879068,
        7.03141202,  9.00323092,  9.04535779,  7.02050836,  6.05051118,
        9.00293775,  7.01100312, 10.00040982, 10.03898511, 10.02261314,
       10.97246529,  9.99173281, 11.01164204, 18.85915407,  8.03103783,
        5.02417693,  6.0891353 , 11.00717545,  5.03709582,  8.00688442,
        8.01904245,  9.05630426, 12.940226  ,  5.036992  ,  7.0018364 ,
        8.99952469, 10.99762393, 11.9342271 ,  7.02996552, 16.90140694,
        4.0351707 ])
#coef
rg.coef_
array([ 1.66542767e-03,  2.04799997e-02,  7.92801891e-03,  3.76048269e-02,
       -6.08725188e-02, -9.13819003e-03,  3.85749358e-02,  3.16069057e+00])
#model building
from sklearn.tree import DecisionTreeClassifier
model=DecisionTreeClassifier()
#fit the data
model.fit(x_train,y_train)
DecisionTreeClassifier()
#prediction
pred2=model.predict(x_test)
## Accuracy of DT model
from sklearn.metrics import accuracy_score
accuracy_score(y_test,pred2)
0.9988038277511961
#confusion matrix 
from sklearn import metrics
metrics.confusion_matrix ( y_test , pred2 )
array([[  5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,  47,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,  98,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0, 113,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0, 107,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,  95,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  66,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         14,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,  10,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   5,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   8,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   8,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   2,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   1,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   2,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   1,   0]], dtype=int64)
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
RandomForestClassifier()
#prediction
pred = rf.predict ( x_test )
## Accuracy of DT model
from sklearn.metrics import accuracy_score
accuracy_score (y_test,pred)
0.9222488038277512
#confusion matrix
from sklearn import metrics
metrics.confusion_matrix ( y_test , pred )
array([[  5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,  10,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,  32,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,  44,   3,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,  95,   3,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   1,   0, 111,   1,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0, 107,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   1,   3,  91,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  66,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  38,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  20,   1,
          2,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   2,  15,
          0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   5,
          4,   1,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   2,
          1,   3,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,
          1,   1,   2,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   3,   0,   0,
          0,   2,   0,   1,   0,   1,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,
          1,   4,   0,   2,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   1,   0,   1,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   1,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,
          1,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          1,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int64)
from sklearn.naive_bayes import GaussianNB
#intitalizing the model
nb=GaussianNB()
#fitting the model on training data
nb.fit ( x_train , y_train )
GaussianNB()
pred=nb.predict(x_test) 
pred
array([13,  8, 11,  5, 12, 11,  7,  8,  7,  9,  8,  8, 11,  9,  4,  7,  7,
       17,  7,  7,  7,  5,  8,  9, 10, 10,  5, 15, 10, 14,  8,  4,  9, 13,
        7,  7,  8,  9,  8, 11, 15, 12, 17, 16, 11, 11,  8, 10, 11,  6, 13,
        7, 13, 20, 12,  7,  8, 10,  7,  7,  9,  9, 11,  8,  7, 12, 13, 17,
        8, 12,  9, 17, 10,  6, 11,  8,  8,  9,  8,  8,  8,  6,  7, 13, 11,
        6,  9, 12,  5,  6, 11, 11,  8,  7, 16,  8, 11,  8, 18, 11, 12, 14,
       12, 11,  6,  9,  7,  6, 11, 11, 11, 12, 20, 10, 14, 10, 10, 12,  4,
       12,  7, 13,  6,  8, 17, 15,  9, 11,  7, 11,  8,  7,  7, 10, 11,  9,
       10,  9,  8, 16, 16, 13,  6,  9, 10,  9,  8,  8,  8,  5,  8, 11,  5,
        9,  9,  6,  8,  7, 10, 11, 12,  8,  9, 10,  5, 16,  7,  6, 15,  9,
       10,  3,  6, 10, 11, 12,  5,  8,  5, 13,  9, 11,  7,  9, 15,  8, 10,
        8,  5,  9, 19,  9,  8,  9, 11,  9,  9,  9,  5, 15, 10, 14, 12,  8,
       12, 11,  5, 10, 11, 23,  4, 11, 11, 10,  9,  9,  9,  8,  5, 13, 12,
       14,  8, 11, 10, 14, 13, 12, 12, 12,  7,  7,  7, 19,  9, 15,  8,  4,
        8,  9, 10, 12,  9,  9,  9,  8, 12,  7, 11, 20,  8,  7, 12, 13, 11,
       14,  6,  8,  9, 11,  9,  8,  9, 11, 20,  9, 12,  7,  6,  7, 11,  7,
        9,  9,  5,  9,  9,  9, 10, 11,  7, 12,  8, 10, 12, 13,  9, 10, 11,
       10,  8,  9,  4,  9,  7,  8,  6,  6,  9, 12, 10,  8, 15, 10, 13, 10,
        8, 10, 14,  8,  9, 12,  8,  8, 12,  9, 10,  7,  8, 10, 11, 14, 10,
       10, 15,  8,  9,  9, 23, 10,  8,  8,  5,  9, 10, 13,  7,  8, 11, 13,
        9, 12, 11, 10, 10,  6,  5, 12, 15,  7,  5, 10, 10, 13, 10, 11, 10,
        9,  7, 13, 10, 10,  7,  9, 13,  5,  6,  9,  7, 11, 11,  7, 14,  7,
       17, 12, 12,  8, 12,  6,  8, 14, 11, 15,  5, 11, 11,  8,  7,  7, 14,
        8, 14, 14, 21,  8, 11,  8, 11, 19,  8,  9,  7, 10,  9, 10,  5, 19,
       16,  9, 10,  7, 11, 12, 15, 11, 20, 12, 13, 10,  8, 10, 11,  9,  5,
       10, 11, 11, 12, 16,  9, 11,  9, 14, 14, 11, 11,  9, 12, 11,  7,  6,
       12,  9,  7,  6, 10,  4, 15, 10,  3,  5,  9,  6,  5, 11, 11,  9,  8,
       13,  7, 14,  7,  8, 12, 10, 11, 14, 16,  6, 10, 10,  8, 10,  8, 11,
        7,  5,  9, 10, 12,  9,  8, 12, 16, 11,  9, 13,  8, 10,  3,  7, 10,
       10,  6,  6,  9,  6, 13,  9,  7, 18,  5, 10,  9,  9,  9, 18,  9,  7,
        9,  8, 18, 15, 10,  8, 12, 12, 15,  8,  8,  9,  9, 10,  6, 16, 10,
       10,  7,  7,  8,  8,  9,  9,  5,  9, 10, 12, 10, 13, 10, 12,  8, 12,
        5,  9,  8,  8, 10, 11, 10, 17, 12, 11,  8,  8,  8,  9,  9, 12, 13,
       12, 10,  9, 13,  6, 11,  9, 15,  8, 13,  8,  8,  8,  9,  8, 10,  9,
       11,  3,  8,  6,  6,  6,  9,  9, 17, 10,  8, 13,  7,  9, 14,  7, 13,
       10,  4, 11,  7, 10,  8, 11, 10, 12, 10,  9,  7, 10, 10, 11,  6, 11,
       12,  9, 13, 12, 12,  7, 13, 16, 11, 11,  6,  8, 12,  9,  7, 12, 10,
       12, 13,  7, 12,  7, 11, 11,  5, 10,  8,  9, 14, 10, 12,  9,  9, 11,
        6, 14, 13, 18,  7,  7, 10,  8,  7,  9,  9,  9, 10, 16, 14,  8, 20,
       15, 13,  8, 10,  7,  6,  9, 10, 10, 12, 11,  5,  8,  8,  7,  7, 10,
       15,  8, 10, 10,  9,  7,  9, 11,  9, 19, 10, 10,  9, 13,  7,  9, 27,
       10,  8,  9,  6, 10, 10, 20,  9,  8, 11,  6, 11, 13,  6,  5,  7,  9,
        8,  7, 14,  8,  7,  7, 20,  4, 10,  6, 13,  7, 11,  9, 19,  9,  7,
       11, 10, 19,  8, 16,  7, 11,  7,  6, 12,  9, 10,  6, 12,  7,  9,  8,
        7, 11,  8,  8,  8, 11, 10, 12,  7,  7,  8, 21,  7,  4,  7,  6, 12,
       10,  9,  7,  9,  6,  7, 11, 11, 11,  8, 11,  6, 22, 10,  9, 17, 14,
        7,  7, 16,  9,  8, 13, 10, 12,  8, 11, 12,  9, 14,  9, 11,  9,  3,
       14, 10,  7,  7,  6,  7,  7,  9,  9,  7,  6,  9,  7, 10, 10, 10, 11,
       10, 11, 19,  8,  5,  6, 11,  5,  8,  8,  9, 13,  5,  7,  9, 11, 12,
        7, 17,  4], dtype=int64)
#evalute model
from sklearn.metrics import accuracy_score , confusion_matrix
accuracy_score ( y_test , pred )
0.9988038277511961
confusion_matrix(y_test , pred )
array([[  5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,  47,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,  98,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0, 113,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0, 107,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,  95,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  66,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         14,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,  10,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   5,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   8,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   8,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   2,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   1,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   2,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   1,   0]], dtype=int64)
 